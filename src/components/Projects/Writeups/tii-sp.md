# **Cloud Infrastructure**

## Website Infrastructure
#### Amazon Lightsail
The Internship Initiative landing page consists of a Wordpress site running on an Apache 2.0 server hosted on the Amazon Lightsail service. Wordpress was an ideal starting point for our operations as the support and ease associated with Wordpress allows us to focus on creating a robust backend infrastructure without allocating too many resources to website maintenance. Likewise, Amazon Lightsail provided us with a cost-effective and low maintenance implementation for website hosting. SSL certification is provided for the interninit.com website through the Let’s Encrypt Organization.

#### EC2
The student and business portals however are hosted on a fleet of EC2 instances within an autoscaling group. Since these applications interact with a host of AWS resources such as load balancers and APIs, it made sense to deploy onto EC2.

#### Amazon Route 53
DNS services and domain name management is provided through the Amazon Route 53 service. Route 53 was an ideal choice for DNS services as it is already embedded into the AWS infrastructure. This allows us to easily integrate AWS services with an existing DNS infrastructure.

## User Information Processing Infrastructure
#### Introduction
In order to apply for an internship position, users must apply through a form located [here](https://apply.interninit.com). Our application form consists of a custom web app written in React JS. In the past, our form provider was CognitoForms which, while easy to use, was not as customizable as we had hoped. CognitoForms provided us with a GUI to view past form information, however, the data was not easily accessible. This forced us to manually review applicants and match them with prospective businesses. With the new User Information Processing Infrastructure, data is handled in a variety of intermediary steps and finally stored in cloud storage on AWS. This allows us to keep our data ready to easily process with any Amazon service that we require.

#### Amazon API Gateway
My first step in building this infrastructure was transferring data from the form application to AWS. Since the form data is already formatted in JSON, creating an API with a public HTTP endpoint was a logical first step. I created a REST API through Amazon’s API Gateway backed by AWS Lambda Functions. The Flask backend serving the React frontend makes HTTP requests to the public API Gateway endpoints in order to facilitate data transfer to and from AWS.

In order to secure this API I decided to use API Gateway's built in integration with Cognito User Pools. Users would authenticate with Cognito through the frontend application and would make API requests with granted JWT tokens. This allowed for selective API access as well as fine-grained access control.

#### Amazon Elasticache
In order to relieve load from the form application, I decided to implement a caching system that stores form data for each user as they progress through the form. The caching system is implemented primarily through the Amazon Elasticache service. My first step in this implementation was provisioning a Multi-AZ Redis cluster secured in an Amazon VPC. Since this cluster spanned multiple AZs, cache data was highly available and secure in case of an AZ failure. Additionally to avoid stale data, optimize cache storage, and increase query time, I decided to implement Lazy Loading and Write Through strategies. This allowed for frequently requested data to populate the cache while ensuring the backing database was always in sync with the cache. Since our application is read heavy, the Write Through strategy did not have a noticable impact on average query times.

#### Amazon Lambda

Lambda functions were primarily used to back methods implemented in the API Gateway REST API. These functions would handle tasks ranging from cache updates to handling file uploads to Amazon S3.

#### Amazon DynamoDB
DynamoDB was chosen as our primary data source for two main reasons. First, since DynamoDB is a NoSQL database, it lends itself to schema-less data. The form and user data generated by our application was varied and unstructed and did not lend itself to standard database schemas found in RDBMSs. As a result we were able to store our data in DynamoDB in a format similar to what our application was expecting. This allowed for much simpler queries and a smaller need for data tranformation to and from the database. Second since DynamoDB is a managed service, we were able to get up and running with DynamoDB extremely fast. There was no need for compute instance provision, security, or management, allowing forvery fast application integration.

#### Amazon Simple Storage Service (S3)
Since file data such as resumés, profile picture, portfolios, etc. were too large to effectively cache, file uploads were handled separately from form data. Uploading a file to any file upload component in React sent a request to the API Gateway containing the raw file data. After the user's identity was confirmed, this file data was written to the user's subfolder in an S3 bucket for long term storage.

# Development Workflow

#### Continuous Integration/Continuous Delivery
In order to create a reliable process for updating AWS resources from changes in source code, I implemented a CI/CD pipeline. Our main motivation for this was to allow for hands free building, testing, and deployment of frontend source code to a fleet of EC2 instances. I decided to utilize the AWS Codepipeline service to implement our CI/CD pipeline. The source action is triggered upon a commit to the master branch of a private GitHub repository. The source artifact is then passed to an Amazon Linux build instance where dependencies are installed and production source code is built. The build artifact is then deployed onto the fleet of EC2 instances within the autoscaling group. To avoid downtime and allow for testing, a blue-green deployment is utilized with the help of an Application Load Balancer.